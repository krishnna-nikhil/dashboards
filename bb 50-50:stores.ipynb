{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ac9fc0b-df25-48be-8aeb-90b1fa43cdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117669 entries, 0 to 117668\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Company          117669 non-null  object        \n",
      " 1   Customer         112071 non-null  object        \n",
      " 2   Order            117669 non-null  object        \n",
      " 3   Order Date       117669 non-null  datetime64[ns]\n",
      " 4   Product Variant  117669 non-null  object        \n",
      " 5   Qty Ordered      117669 non-null  float64       \n",
      " 6   Salesperson      117627 non-null  object        \n",
      " 7   Total            117669 non-null  float64       \n",
      " 8   Unit Price       117669 non-null  float64       \n",
      " 9   Customer/Phone   5935 non-null    object        \n",
      " 10  Customer/Mobile  75277 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(7)\n",
      "memory usage: 9.9+ MB\n",
      "['BRYT BAZAAR ITI' 'BRYT BAZAAR J P NAGAR' 'BRYT BAZAAR RK HEGDE NAGAR'\n",
      " 'BRYT BAZAAR BASAPURA MAIN ROAD' 'BRYT BAZAAR RAJAJINAGAR']\n",
      "['ITI' 'JPN' 'RKH' 'BMR' 'RJJ']\n",
      "Specified products have been removed from the DataFrame.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 117669 entries, 0 to 117668\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count   Dtype         \n",
      "---  ------           --------------   -----         \n",
      " 0   Company          117669 non-null  object        \n",
      " 1   Customer         112071 non-null  object        \n",
      " 2   Order            117669 non-null  object        \n",
      " 3   Order Date       117669 non-null  datetime64[ns]\n",
      " 4   Product Variant  117669 non-null  object        \n",
      " 5   Qty Ordered      117669 non-null  float64       \n",
      " 6   Salesperson      117627 non-null  object        \n",
      " 7   Total            117669 non-null  float64       \n",
      " 8   Unit Price       117669 non-null  float64       \n",
      " 9   Customer/Phone   5935 non-null    object        \n",
      " 10  Customer/Mobile  75277 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(3), object(7)\n",
      "memory usage: 9.9+ MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_73892/2995814277.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Total'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the Excel file\n",
    "file_path = \"/Users/devnikhil/Downloads/Sales Analysis Report (sale.report) - 2025-06-28T175844.229.xlsx\"\n",
    "\n",
    "# Load the Excel file into a DataFrame\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create the 'fifty_disc' column based on the condition for 'Discount %'\n",
    "\n",
    "df['Total'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Display unique companies\n",
    "unique_companies = df['Company'].unique()\n",
    "\n",
    "# Print the unique companies\n",
    "print(unique_companies)\n",
    "\n",
    "# Filter out 'BRYT BAZAAR -BLR' and 'BRYT BAZAAR RICHMOND TOWN'\n",
    "df = df[~df['Company'].isin(['BRYT BAZAAR -BLR','Bryt Bazaar Indiranagar', 'BRYT BAZAAR RICHMOND TOWN','SRI DEEPA RETAIL','DEEPA RETAIL','BRYT BAZAAR YELLAHANKA'])]\n",
    "\n",
    "# Rename specific company names\n",
    "df['Company'] = df['Company'].replace({\n",
    "    'BRYT BAZAAR J P NAGAR': 'JPN',\n",
    "    'BRYT BAZAAR ITI': 'ITI',\n",
    "    'BRYT BAZAAR KUMARSWAMY LAYOUT': 'KSL',\n",
    "    'BRYT BAZAAR BASAPURA MAIN ROAD': 'BMR',\n",
    "    'BRYT BAZAAR RAJAJINAGAR': 'RJJ',\n",
    "    'BRYT BAZAAR RK HEGDE NAGAR': 'RKH',\n",
    "    'Bryt Bazaar Indiranagar': 'IND'\n",
    "})\n",
    "\n",
    "# Optional: Print to verify the changes\n",
    "print(df['Company'].unique())\n",
    "df = df[df['Customer'] != 'SRI DEEPA RETAIL']\n",
    "\n",
    "df = df[df['Customer'] != 'KIKO']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Specified products have been removed from the DataFrame.\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1a089111-d554-4a04-8dd8-e098494d698e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4913</th>\n",
       "      <td>2025-05-18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>2025-05-19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4748</th>\n",
       "      <td>2025-05-20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4626</th>\n",
       "      <td>2025-05-21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>2025-05-22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4370</th>\n",
       "      <td>2025-05-23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4156</th>\n",
       "      <td>2025-05-24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4008</th>\n",
       "      <td>2025-05-25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3932</th>\n",
       "      <td>2025-05-26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3808</th>\n",
       "      <td>2025-05-27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3730</th>\n",
       "      <td>2025-05-28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3643</th>\n",
       "      <td>2025-05-29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>2025-05-30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3345</th>\n",
       "      <td>2025-05-31</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>2025-06-01</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>2025-06-02</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>2025-06-03</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2216</th>\n",
       "      <td>2025-06-04</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>2025-06-05</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>2025-06-06</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>2025-06-07</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>2025-06-12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>2025-06-13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-06-14</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  week\n",
       "4913  2025-05-18     1\n",
       "4842  2025-05-19     1\n",
       "4748  2025-05-20     1\n",
       "4626  2025-05-21     1\n",
       "4519  2025-05-22     1\n",
       "4370  2025-05-23     1\n",
       "4156  2025-05-24     1\n",
       "4008  2025-05-25     2\n",
       "3932  2025-05-26     2\n",
       "3808  2025-05-27     2\n",
       "3730  2025-05-28     2\n",
       "3643  2025-05-29     2\n",
       "3500  2025-05-30     2\n",
       "3345  2025-05-31     2\n",
       "3226  2025-06-01     3\n",
       "3168  2025-06-02     3\n",
       "3037  2025-06-03     3\n",
       "2216  2025-06-04     3\n",
       "1077  2025-06-05     3\n",
       "878   2025-06-06     3\n",
       "774   2025-06-07     3\n",
       "620   2025-06-12     4\n",
       "150   2025-06-13     4\n",
       "0     2025-06-14     4"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure 'Order Date' is in datetime format\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'])\n",
    "\n",
    "# Split 'Order Date' into separate 'date' and 'time' columns\n",
    "df['date'] = df['Order Date'].dt.date\n",
    "\n",
    "# Determine the earliest date\n",
    "earliest_date = df['date'].min()\n",
    "\n",
    "# Calculate the 'week' column starting from the earliest date\n",
    "df['week'] = ((pd.to_datetime(df['date']) - pd.to_datetime(earliest_date)).dt.days // 7) + 1\n",
    "\n",
    "# Display the updated DataFrame\n",
    "df[['date', 'week']].drop_duplicates().sort_values(by='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "355f728b-44aa-494d-a55e-41413370c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range  week  <500  500-999  1000-1999  2000-2999  3000+\n",
      "0         1   255       22          8          0      1\n",
      "1         2   252       20         10          1      1\n",
      "2         3   305       43         24          9     12\n",
      "3         4    56       10         10          4      6\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get unique bills per Order Date\n",
    "bill_summary = df.groupby('Order Date', as_index=False)['Total'].sum()\n",
    "\n",
    "# Step 2: Merge to get week info\n",
    "week_map = df[['Order Date', 'week']].drop_duplicates()\n",
    "bill_summary = bill_summary.merge(week_map, on='Order Date', how='left')\n",
    "\n",
    "# Step 3: Define proper range buckets\n",
    "def classify_range(total):\n",
    "    if total < 500:\n",
    "        return '<500'\n",
    "    elif 500 <= total < 1000:\n",
    "        return '500-999'\n",
    "    elif 1000 <= total < 2000:\n",
    "        return '1000-1999'\n",
    "    elif 2000 <= total < 3000:\n",
    "        return '2000-2999'\n",
    "    else:\n",
    "        return '3000+'\n",
    "\n",
    "bill_summary['Range'] = bill_summary['Total'].apply(classify_range)\n",
    "\n",
    "# Step 4: Group by week and range\n",
    "weekly_summary = (\n",
    "    bill_summary.groupby(['week', 'Range'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 5: Ensure column order\n",
    "col_order = ['week', '<500', '500-999', '1000-1999', '2000-2999', '3000+']\n",
    "for col in col_order:\n",
    "    if col not in weekly_summary.columns:\n",
    "        weekly_summary[col] = 0\n",
    "weekly_summary = weekly_summary[col_order]\n",
    "\n",
    "# Output\n",
    "print(weekly_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a726e30e-10fe-46e7-a2d0-d949f05f0afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Order Dates with bills above 3000 in week 4:\n",
      "              Order Date   Total\n",
      "1011 2025-06-13 21:51:34  5133.6\n",
      "1001 2025-06-13 17:26:22  4706.4\n",
      "998  2025-06-13 16:19:56  4559.6\n",
      "1005 2025-06-13 18:31:12  4217.0\n",
      "967  2025-06-12 21:54:35  3476.5\n",
      "1041 2025-06-14 18:37:25  3300.3\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get unique bills per Order Date\n",
    "bill_summary = df.groupby('Order Date', as_index=False)['Total'].sum()\n",
    "\n",
    "# Step 2: Merge week info\n",
    "week_map = df[['Order Date', 'week']].drop_duplicates()\n",
    "bill_summary = bill_summary.merge(week_map, on='Order Date', how='left')\n",
    "\n",
    "# Step 3: Filter for week 4 and Total > 3000\n",
    "week_4_above_3000 = bill_summary[(bill_summary['week'] == 4) & (bill_summary['Total'] >= 3000)]\n",
    "\n",
    "# Step 4: Display unique order dates\n",
    "print(\"Unique Order Dates with bills above 3000 in week 4:\")\n",
    "print(week_4_above_3000[['Order Date', 'Total']].sort_values(by='Total', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec2f56-d115-42c4-90fb-f8f684c00cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e52f98-9916-404e-ad61-74c724944d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "ee4c6c8f-75fa-4005-9cd0-ba7c5a2da72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top sku\n",
      "                    Product Variant  Total Sales  Total Quantity Ordered\n",
      "Sunpure Refined Sunflower Oil 1 ltr       2035.0                      19\n",
      "total sales\n",
      "Company    Sales\n",
      "    JPN 34566.45\n",
      "    RKH 16221.60\n",
      "    ITI  9359.80\n",
      "    BMR  5611.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filter data for Week 4\n",
    "week_4_data = df[df['week'] == 4]\n",
    "\n",
    "# Find the most-selling SKU for Week 4\n",
    "most_selling_sku = week_4_data.groupby('Product Variant')['Total'].sum().idxmax()\n",
    "most_selling_total = week_4_data.groupby('Product Variant')['Total'].sum().max()\n",
    "\n",
    "# Calculate total quantity ordered for the most-selling SKU\n",
    "most_selling_sku_quantity = week_4_data[week_4_data['Product Variant'] == most_selling_sku]['Qty Ordered'].sum()\n",
    "\n",
    "# Format the most-selling SKU data into a DataFrame\n",
    "most_selling_sku_data = pd.DataFrame({\n",
    "    'Product Variant': [most_selling_sku],\n",
    "    'Total Sales': [most_selling_total],\n",
    "    'Total Quantity Ordered': [most_selling_sku_quantity]\n",
    "})\n",
    "\n",
    "# Calculate total sales by company and format into a DataFrame\n",
    "branch_total_sales = week_4_data.groupby('Company')['Total'].sum().sort_values(ascending=False)\n",
    "branch_total_sales_df = branch_total_sales.reset_index()\n",
    "branch_total_sales_df.columns = ['Company', 'Sales']\n",
    "\n",
    "# Combine and display results\n",
    "print(\"top sku\")\n",
    "print(most_selling_sku_data.to_string(index=False))\n",
    "\n",
    "print(\"total sales\")\n",
    "print(branch_total_sales_df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3e05997c-4518-489e-aa1e-c26ff7d65d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "week Company     Week 3    Week 4  Difference  Percentage_Change\n",
      "0        JPN  157473.35  34566.45  -122906.90         -78.049333\n",
      "1        KSL        NaN       NaN         NaN                NaN\n",
      "2        BMR   18096.45   5611.00   -12485.45         -68.993919\n",
      "3        ITI    5419.80   9359.80     3940.00          72.696409\n",
      "4        RKH    7728.10  16221.60     8493.50         109.904116\n",
      "5        RJJ      55.00       NaN         NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "comparison_df = df[df['week'].isin([3, 4])].groupby(['Company', 'week'])['Total'].sum().unstack().assign(\n",
    "    Difference=lambda x: x[4] - x[3],\n",
    "    Percentage_Change=lambda x: (x[4] - x[3]) / x[3] * 100\n",
    ").reset_index().rename(columns={3: 'Week 3', 4: 'Week 4', 'Change': 'Difference (W4-W3)', 'Change %': 'Percentage Change (%)'})\n",
    "# Reset index and reorder based on a custom list of company names\n",
    "custom_order = ['JPN', 'KSL', 'BMR', 'ITI', 'RKH', 'RJJ']\n",
    "comparison_df = comparison_df.set_index('Company').reindex(custom_order).reset_index()\n",
    "\n",
    "# Display the final DataFrame\n",
    "print(comparison_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a6cb6041-c085-4985-8cae-b63fc92f9cd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['KSL'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Reorder the rows as per the desired order: 'JPN', 'KSL', 'BMR', 'ITI', 'RJJ', 'RKH'\u001b[39;00m\n\u001b[1;32m     59\u001b[0m ordered_companies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKSL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRKH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRJJ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m weekly_summary_df \u001b[38;5;241m=\u001b[39m weekly_summary_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[ordered_companies]\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Print the final DataFrame\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(weekly_summary_df)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['KSL'] not in index\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' is your DataFrame that contains the sales data\n",
    "# Example: df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Initialize an empty list to store summary rows\n",
    "weekly_summary_data = []\n",
    "\n",
    "# Define the column names for the summary DataFrame\n",
    "columns = [\n",
    "    \"Company\",\n",
    "    \"Total Sales W1\", \"Total Sales W2\", \"Total Sales W3\", \"Total Sales W4\",\n",
    "    \"AOV W1\", \"AOV W2\", \"AOV W3\", \"AOV W4\",\n",
    "    \"NOB W1\", \"NOB W2\", \"NOB W3\", \"NOB W4\",\n",
    "    \"ADS W1\", \"ADS W2\", \"ADS W3\", \"ADS W4\"  # Added Average Daily Sales (ADS)\n",
    "]\n",
    "\n",
    "# Iterate over each unique company in the dataset\n",
    "for company in df['Company'].unique():\n",
    "    # Create a list to hold data for each company\n",
    "    company_data = [company]\n",
    "    \n",
    "    # Calculate Total Sales, AOV, and NOB for each week\n",
    "    total_sales = []\n",
    "    aov = []\n",
    "    nob = []\n",
    "    ads = []  # For Average Daily Sales (ADS)\n",
    "    \n",
    "    for week in range(1, 5):  # Iterate over weeks 1 to 4\n",
    "        # Filter the data for the current company and week\n",
    "        weekly_data = df[(df['Company'] == company) & (df['week'] == week)]\n",
    "        \n",
    "        # Calculate Total Sales for the current week\n",
    "        week_sales = weekly_data['Total'].sum()\n",
    "        total_sales.append(week_sales)\n",
    "        \n",
    "        # Calculate NOB (Number of Unique Order Dates) for the current week\n",
    "        unique_order_dates = weekly_data['Order Date'].nunique()\n",
    "        nob.append(unique_order_dates)\n",
    "        \n",
    "        # Calculate AOV for the current week\n",
    "        week_aov = week_sales / unique_order_dates if unique_order_dates > 0 else 0\n",
    "        aov.append(week_aov)\n",
    "        \n",
    "        # Calculate Average Daily Sales (ADS) for the current week\n",
    "        week_ads = week_sales / 7  # Assuming 7 days in a week\n",
    "        ads.append(week_ads)\n",
    "    \n",
    "    # Append the calculated data for the current company to the list\n",
    "    company_data.extend(total_sales + aov + nob + ads)\n",
    "    \n",
    "    # Append the company data to the summary list\n",
    "    weekly_summary_data.append(company_data)\n",
    "\n",
    "# Convert the summary data to a DataFrame\n",
    "weekly_summary_df = pd.DataFrame(weekly_summary_data, columns=columns)\n",
    "\n",
    "# Reorder the rows as per the desired order: 'JPN', 'KSL', 'BMR', 'ITI', 'RJJ', 'RKH'\n",
    "ordered_companies = ['JPN', 'KSL', 'BMR', 'ITI', 'RKH', 'RJJ']\n",
    "weekly_summary_df = weekly_summary_df.set_index('Company').loc[ordered_companies].reset_index()\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(weekly_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "320f874a-b47c-416f-9a22-ac831b7079e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/2685224917.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_week_4['fifty_disc'] = df_week_4['Discount %'].apply(lambda x: 'yes' if x >= 50 else 'no')\n",
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/2685224917.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  company_sales_summary_week_4 = df_week_4.groupby('Company').apply(lambda group: pd.Series({\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['KSL', 'RJJ'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[153], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Step 5: Reorder the companies as per the specified order: 'JPN', 'KSL', 'BMR', 'ITI', 'RKH', 'RJJ'\u001b[39;00m\n\u001b[1;32m     24\u001b[0m ordered_companies \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJPN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKSL\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBMR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mITI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRKH\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRJJ\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m company_sales_summary_week_4 \u001b[38;5;241m=\u001b[39m company_sales_summary_week_4\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompany\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mloc[ordered_companies]\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Print the updated DataFrame with the percentages and the ordered companies\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(company_sales_summary_week_4)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_axis(maybe_callable, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1420\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1418\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index with multidimensional key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_iterable(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1422\u001b[0m \u001b[38;5;66;03m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1360\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1359\u001b[0m \u001b[38;5;66;03m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1362\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_dups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1363\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexing.py:1558\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1555\u001b[0m ax \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1556\u001b[0m axis_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1558\u001b[0m keyarr, indexer \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1560\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['KSL', 'RJJ'] not in index\""
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the DataFrame for week == 4\n",
    "df_week_4 = df[df['week'] == 4]\n",
    "\n",
    "# Step 2: Create the 'fifty_disc' column to categorize whether the discount is 50% or more\n",
    "df_week_4['fifty_disc'] = df_week_4['Discount %'].apply(lambda x: 'yes' if x >= 50 else 'no')\n",
    "\n",
    "# Step 3: Group by company and calculate the total sales, total normal sales, and total 50% discount sales for week 4\n",
    "company_sales_summary_week_4 = df_week_4.groupby('Company').apply(lambda group: pd.Series({\n",
    "    'Total Sales': group['Total'].sum(),\n",
    "    'Total Normal Sales': group[group['fifty_disc'] == 'no']['Total'].sum(),\n",
    "    'Total 50% Discount Sales': group[group['fifty_disc'] == 'yes']['Total'].sum()\n",
    "})).reset_index()\n",
    "\n",
    "# Step 4: Calculate the percentages for normal and 50% discount sales\n",
    "company_sales_summary_week_4['Total Normal Sales %'] = (\n",
    "    company_sales_summary_week_4['Total Normal Sales'] / company_sales_summary_week_4['Total Sales'] * 100\n",
    ")\n",
    "\n",
    "company_sales_summary_week_4['Total 50% Discount Sales %'] = (\n",
    "    company_sales_summary_week_4['Total 50% Discount Sales'] / company_sales_summary_week_4['Total Sales'] * 100\n",
    ")\n",
    "\n",
    "# Step 5: Reorder the companies as per the specified order: 'JPN', 'KSL', 'BMR', 'ITI', 'RKH', 'RJJ'\n",
    "ordered_companies = ['JPN', 'KSL', 'BMR', 'ITI', 'RKH', 'RJJ']\n",
    "company_sales_summary_week_4 = company_sales_summary_week_4.set_index('Company').loc[ordered_companies].reset_index()\n",
    "\n",
    "# Print the updated DataFrame with the percentages and the ordered companies\n",
    "print(company_sales_summary_week_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fee337da-6a0b-43f6-b766-eea34628a6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Company  Total Sales  Total Normal Sales  Total 50% Discount Sales  \\\n",
      "0     BMR      5611.00             1532.00                    4079.0   \n",
      "1     ITI      9359.80             5801.00                    3558.8   \n",
      "2     JPN     34566.45            19002.75                   15563.7   \n",
      "3     RKH     16221.60             5454.50                   10767.1   \n",
      "\n",
      "   Total Normal Sales %  Total 50% Discount Sales %  \n",
      "0             27.303511                   72.696489  \n",
      "1             61.977820                   38.022180  \n",
      "2             54.974549                   45.025451  \n",
      "3             33.624920                   66.375080  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/3751078043.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_week_4['fifty_disc'] = df_week_4['Discount %'].apply(lambda x: 'yes' if x >= 50 else 'no')\n",
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/3751078043.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  company_sales_summary_week_4 = df_week_4.groupby('Company').apply(lambda group: pd.Series({\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the DataFrame for week == 4\n",
    "df_week_4 = df[df['week'] == 4]\n",
    "\n",
    "# Step 2: Create the 'fifty_disc' column to categorize whether the discount is 50% or more\n",
    "df_week_4['fifty_disc'] = df_week_4['Discount %'].apply(lambda x: 'yes' if x >= 50 else 'no')\n",
    "\n",
    "# Step 3: Group by company and calculate the total sales, total normal sales, and total 50% discount sales for week 4\n",
    "company_sales_summary_week_4 = df_week_4.groupby('Company').apply(lambda group: pd.Series({\n",
    "    'Total Sales': group['Total'].sum(),\n",
    "    'Total Normal Sales': group[group['fifty_disc'] == 'no']['Total'].sum(),\n",
    "    'Total 50% Discount Sales': group[group['fifty_disc'] == 'yes']['Total'].sum()\n",
    "})).reset_index()\n",
    "\n",
    "# Step 4: Calculate the percentages for normal and 50% discount sales\n",
    "company_sales_summary_week_4['Total Normal Sales %'] = (\n",
    "    company_sales_summary_week_4['Total Normal Sales'] / company_sales_summary_week_4['Total Sales'] * 100\n",
    ")\n",
    "\n",
    "company_sales_summary_week_4['Total 50% Discount Sales %'] = (\n",
    "    company_sales_summary_week_4['Total 50% Discount Sales'] / company_sales_summary_week_4['Total Sales'] * 100\n",
    ")\n",
    "\n",
    "# Step 5: Print the result as is (no reordering)\n",
    "print(company_sales_summary_week_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c2c2d73d-cdef-493e-ba16-8f449782b5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        Product Variant  Revenue Loss\n",
      "16                                    70% on your order        7700.7\n",
      "1440             Sunpure Refined sunflower oil 500 ml          3382.0\n",
      "73          Ariel Matic Front Load Detergent Powder 2kg        3300.0\n",
      "1020                                 Mysore Sandal Soap        2952.0\n",
      "1022                       Mysore Sandal Soap gold 125g        2730.0\n",
      "1023                          Mysore Sandal Soap125 gms        2373.6\n",
      "1014               Mysore Sandal Centennial Soap100 gms        2250.0\n",
      "1314                                        SURF EXCELL        2037.0\n",
      "1021                             Mysore Sandal Soap 75g        2037.0\n",
      "1577                                       Vessel@2000         1999.0\n",
      "91                                        B-Kids cloths        1950.0\n",
      "875                        MYSORE SANDAL SOAD GOLD 125G        1926.0\n",
      "1024      Mysore Sandal Superior Soap 150 g (Pack of 3)        1822.5\n",
      "360                      Daawat Super Basmati Rice 1 kg        1772.0\n",
      "313   Colgate Strong Teeth Anticavity Toothpaste Wit...        1716.0\n",
      "971   Milton Mirage 1000 Thermosteel Hot and Cold Wa...        1650.0\n",
      "301                              Colgate Max Fresh 150g        1650.0\n",
      "970   Milton Hamper Laundry/Toy Organizer Basket - B...        1580.0\n",
      "1091      Nivea Body Milk Shea Smooth Deep Moisture Ser        1550.0\n",
      "1028               Mysore Sandal jasmine rose soap 450g        1540.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data for Week 1 and Week 2\n",
    "week_1_data = df[df['week'] == 3]\n",
    "week_2_data = df[df['week'] == 4]\n",
    "\n",
    "# Aggregate sales data by 'Product Variant' for Week 1\n",
    "week_1_aggregated = week_1_data.groupby('Product Variant', as_index=False)['Total'].sum()\n",
    "\n",
    "# Aggregate sales data by 'Product Variant' for Week 2\n",
    "week_2_aggregated = week_2_data.groupby('Product Variant', as_index=False)['Total'].sum()\n",
    "\n",
    "# Merge the aggregated data for Week 1 and Week 2\n",
    "comparison_data = pd.merge(week_1_aggregated, \n",
    "                           week_2_aggregated, \n",
    "                           on='Product Variant', \n",
    "                           how='outer', \n",
    "                           suffixes=('_Week1', '_Week2'))\n",
    "\n",
    "# Fill NaN values with 0 for products that might be missing in either week\n",
    "comparison_data.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the revenue loss between Week 1 and Week 2 (Sales in Week 1 - Sales in Week 2)\n",
    "comparison_data['Revenue Loss'] = comparison_data['Total_Week1'] - comparison_data['Total_Week2']\n",
    "\n",
    "# Sort the data by 'Revenue Loss' to find the most significant losses (in descending order)\n",
    "comparison_data_sorted = comparison_data.sort_values(by='Revenue Loss', ascending=False)\n",
    "\n",
    "# Filter for the top 50 products with the most significant revenue loss\n",
    "top_50_revenue_loss_products = comparison_data_sorted.head(20)\n",
    "\n",
    "# Drop duplicates based on 'Product Variant' to keep only one instance of each product\n",
    "top_50_revenue_loss_products_unique = top_50_revenue_loss_products.drop_duplicates(subset='Product Variant')\n",
    "\n",
    "# Display the filtered top 50 products with the most significant revenue loss from Week 1 to Week 2\n",
    "print(top_50_revenue_loss_products_unique[['Product Variant', 'Revenue Loss']])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8c7c3475-a57d-4082-a78c-63b4b5daed09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Product Variant  Revenue Gain\n",
      "11                           50% on your order      100352.5\n",
      "1310                     SRI KRISHNA GHEE 1LTR        2004.0\n",
      "1822                        zebronics zeb bang        1599.0\n",
      "388                                   Discount        1330.2\n",
      "1460                             Syska HT 1200        1299.0\n",
      "1655                                       bag        1240.0\n",
      "1444  Sunsilk Stunning Black Shine Shampoo 1 L         999.0\n",
      "176   Borosil Presto Aluminium Pressure Cooker         853.5\n",
      "1578                               Vessel@300          837.2\n",
      "1462                              Syska HT200u         829.0\n",
      "1122      Origami Kitchen Towel Roll pack of 4         825.0\n",
      "1640                                  Zeb Bang         799.5\n",
      "1156   Pantene Shampoo Silky Smooth Care 340ml         750.0\n",
      "453            Engage Ocean Zest For him 150ml         747.0\n",
      "1691                          fab paint spray          735.0\n",
      "1461                              Syska HT1309         649.5\n",
      "1469                      TATA GOLD TEA POWDER         630.0\n",
      "133                             Bajaj Dosti 3W         600.0\n",
      "1642                                  Zeb veta         599.7\n",
      "995                      Morphy Richards HD031         598.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Filter data for Week 1 and Week 2\n",
    "week_1_data = df[df['week'] == 3]\n",
    "week_2_data = df[df['week'] == 4]\n",
    "\n",
    "# Aggregate sales data by 'Product Variant' for Week 1\n",
    "week_1_aggregated = week_1_data.groupby('Product Variant', as_index=False)['Total'].sum()\n",
    "\n",
    "# Aggregate sales data by 'Product Variant' for Week 2\n",
    "week_2_aggregated = week_2_data.groupby('Product Variant', as_index=False)['Total'].sum()\n",
    "\n",
    "# Merge the aggregated data for Week 1 and Week 2\n",
    "comparison_data = pd.merge(\n",
    "    week_1_aggregated, \n",
    "    week_2_aggregated, \n",
    "    on='Product Variant', \n",
    "    how='outer', \n",
    "    suffixes=('_Week1', '_Week2')\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0 for products that might be missing in either week\n",
    "comparison_data.fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the revenue gain between Week 1 and Week 2 (Sales in Week 2 - Sales in Week 1)\n",
    "comparison_data['Revenue Gain'] = comparison_data['Total_Week2'] - comparison_data['Total_Week1']\n",
    "\n",
    "# Filter products where the revenue gain is positive (i.e., Week 2 sales > Week 1 sales)\n",
    "positive_revenue_gain = comparison_data[comparison_data['Revenue Gain'] > 0]\n",
    "\n",
    "# Sort the data by 'Revenue Gain' to find the most significant gains (in descending order)\n",
    "positive_revenue_gain_sorted = positive_revenue_gain.sort_values(by='Revenue Gain', ascending=False)\n",
    "\n",
    "# Filter for the top 50 products with the most significant revenue gain\n",
    "top_50_revenue_gain_products = positive_revenue_gain_sorted.head(20)\n",
    "\n",
    "# Display the filtered top 50 products with the most significant revenue gain from Week 1 to Week 2\n",
    "print(top_50_revenue_gain_products[['Product Variant', 'Revenue Gain']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "30be6ae8-05b7-42cd-a647-8332d1117b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 3 Company  Total NoB  14 Qty (%)  59 Qty (%)  10+ Qty (%)\n",
      "0          BMR          9        22.22        11.11        66.67\n",
      "1          ITI          8        25.00        25.00        50.00\n",
      "2          JPN         54        48.15        16.67        35.19\n",
      "3          RKH         15        53.33         6.67        40.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/1787493387.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped.groupby(['Company', 'WEEK 3'])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the data for week 3\n",
    "week_3_df = df[df['week'] == 4]\n",
    "\n",
    "# Step 2: Group by 'Company' and 'Order Date' to calculate total quantity per bill\n",
    "grouped = (\n",
    "    week_3_df.groupby(['Company', 'Order Date'])\n",
    "    .agg({'Qty Ordered': 'sum'})  # Sum the quantities per bill\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 3: Categorize bills based on total quantity sold\n",
    "grouped['WEEK 3'] = pd.cut(\n",
    "    grouped['Qty Ordered'],\n",
    "    bins=[0, 4, 9, float('inf')],\n",
    "    labels=['14 Qty', '59 Qty', '10+ Qty'],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# Step 4: Calculate total bills and categorize counts for each company\n",
    "summary = (\n",
    "    grouped.groupby(['Company', 'WEEK 3'])\n",
    "    .size()  # Count the number of bills in each category\n",
    "    .unstack(fill_value=0)  # Transform the categories into columns\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 5: Calculate total bills per company\n",
    "summary['Total NoB'] = summary[['14 Qty', '59 Qty', '10+ Qty']].sum(axis=1)\n",
    "\n",
    "# Step 6: Calculate percentages for each category\n",
    "summary['14 Qty (%)'] = (summary['14 Qty'] / summary['Total NoB'] * 100).round(2)\n",
    "summary['59 Qty (%)'] = (summary['59 Qty'] / summary['Total NoB'] * 100).round(2)\n",
    "summary['10+ Qty (%)'] = (summary['10+ Qty'] / summary['Total NoB'] * 100).round(2)\n",
    "\n",
    "# Step 7: Reorganize columns for clarity\n",
    "summary = summary[['Company', 'Total NoB', '14 Qty (%)', '59 Qty (%)', '10+ Qty (%)']]\n",
    "\n",
    "# Step 8: Display the summary\n",
    "print(summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "960f9b32-f5a6-4c8d-b7ad-a79cf047802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEEK 4 Company  Total NoB  14 Qty (%)  59 Qty (%)  10+ Qty (%)\n",
      "0          BMR         94        76.60         8.51        14.89\n",
      "1          ITI         10        50.00        20.00        30.00\n",
      "2          JPN        254        56.30        13.78        29.92\n",
      "3          RJJ          1       100.00         0.00         0.00\n",
      "4          RKH         33        66.67        15.15        18.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/1553752761.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  grouped.groupby(['Company', 'WEEK 4'])\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the data for week 3\n",
    "week_3_df = df[df['week'] == 3]\n",
    "\n",
    "# Step 2: Group by 'Company' and 'Order Date' to calculate total quantity per bill\n",
    "grouped = (\n",
    "    week_3_df.groupby(['Company', 'Order Date'])\n",
    "    .agg({'Qty Ordered': 'sum'})  # Sum the quantities per bill\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 3: Categorize bills based on total quantity sold\n",
    "grouped['WEEK 4'] = pd.cut(\n",
    "    grouped['Qty Ordered'],\n",
    "    bins=[0, 4, 9, float('inf')],\n",
    "    labels=['14 Qty', '59 Qty', '10+ Qty'],\n",
    "    right=True\n",
    ")\n",
    "\n",
    "# Step 4: Calculate total bills and categorize counts for each company\n",
    "summary1 = (\n",
    "    grouped.groupby(['Company', 'WEEK 4'])\n",
    "    .size()  # Count the number of bills in each category\n",
    "    .unstack(fill_value=0)  # Transform the categories into columns\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Step 5: Calculate total bills per company\n",
    "summary1['Total NoB'] = summary1[['14 Qty', '59 Qty', '10+ Qty']].sum(axis=1)\n",
    "\n",
    "# Step 6: Calculate percentages for each category\n",
    "summary1['14 Qty (%)'] = (summary1['14 Qty'] / summary1['Total NoB'] * 100).round(2)\n",
    "summary1['59 Qty (%)'] = (summary1['59 Qty'] / summary1['Total NoB'] * 100).round(2)\n",
    "summary1['10+ Qty (%)'] = (summary1['10+ Qty'] / summary1['Total NoB'] * 100).round(2)\n",
    "\n",
    "# Step 7: Reorganize columns for clarity\n",
    "summary1 = summary1[['Company', 'Total NoB', '14 Qty (%)', '59 Qty (%)', '10+ Qty (%)']]\n",
    "\n",
    "# Step 8: Display the summary\n",
    "print(summary1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b128c188-b483-4d0d-a5a9-49eb84a53158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Total  Qty Ordered\n",
      "Product Variant                                                        \n",
      "Sunpure Refined Sunflower Oil 1 ltr                 2035.0           19\n",
      "SRI KRISHNA GHEE 1LTR                               2004.0            3\n",
      "zebronics zeb bang                                  1599.0            1\n",
      "Sunpure Refined sunflower oil 500 ml                1529.5           31\n",
      "Vessel@300                                          1435.2            6\n",
      "Syska HT 1200                                       1299.0            1\n",
      "bag                                                 1240.0            4\n",
      "Dove Hair Fall Rescue Shampoo 1 L                   1050.0            1\n",
      "Sunsilk Stunning Black Shine Shampoo 1 L             999.0            1\n",
      "Syska HT700                                          959.4            2\n",
      "FLOO MAT                                             945.0            4\n",
      "Borosil Presto Aluminium Pressure Cooker             853.5            1\n",
      "Syska HT200u                                         829.0            1\n",
      "Origami Kitchen Towel Roll pack of 4                 825.0            3\n",
      "Colgate Strong Teeth Anticavity Toothpaste With...   804.0            3\n",
      "Zeb Bang                                             799.5            1\n",
      "Pantene Shampoo Silky Smooth Care 340ml              750.0            2\n",
      "Engage Ocean Zest For him 150ml                      747.0            3\n",
      "fab paint spray                                      735.0            3\n",
      "Whisper Bindazzz Nights Sanitary Pads XXXL , 20...   700.0            1\n",
      "Empty DataFrame\n",
      "Columns: [Total, Qty Ordered]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame and 'week' is already a column in your data\n",
    "\n",
    "# Filter data for the 4th week\n",
    "fourth_week_data = df[df['week'] == 4]\n",
    "\n",
    "# Group by 'Product Variant' and aggregate 'Total' sales and 'Qty Ordered'\n",
    "top_products_fourth_week = fourth_week_data.groupby('Product Variant').agg({'Total': 'sum', 'Qty Ordered': 'sum'}).sort_values(by='Total', ascending=False).head(20)\n",
    "\n",
    "# Print the top 5 products based on total sales for the 4th week\n",
    "print(top_products_fourth_week)\n",
    "\n",
    "\n",
    "# Filter data for the 4th week and 'RJJ' company\n",
    "rjj_data = df[(df['Company'] == 'RJJ') & (df['week'] == 4)]\n",
    "\n",
    "# Group by 'Product Variant' and aggregate 'Total' sales and 'Qty Ordered'\n",
    "top_products_rjj = (\n",
    "    rjj_data.groupby('Product Variant')\n",
    "    .agg({'Total': 'sum', 'Qty Ordered': 'sum'})\n",
    "    .sort_values(by='Total', ascending=False)\n",
    "    .head(20)\n",
    ")\n",
    "\n",
    "# Print the top 25 products based on total sales for the company 'RJJ'\n",
    "print(top_products_rjj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dc298f6e-feaa-42f8-9c92-b232ce985dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully added to /Users/devnikhil/Downloads/juneweek2.xlsx.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/zbw4m6rd06dgnzjjvcx1pjr00000gn/T/ipykernel_10393/3008873689.py:23: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  skus_per_bill = total_quantity_sold/number_of_bills\n"
     ]
    }
   ],
   "source": [
    "from openpyxl import Workbook\n",
    "\n",
    "file_path = \"/Users/devnikhil/Downloads/juneweek2.xlsx\"\n",
    "wb = Workbook()\n",
    "\n",
    "# Remove the default sheet created by openpyxl\n",
    "default_sheet = wb.active\n",
    "wb.remove(default_sheet)\n",
    "\n",
    "# Get the list of unique branches\n",
    "unique_branches = df['Company'].unique()\n",
    "\n",
    "# Iterate through each branch and calculate metrics for week 6\n",
    "for branch in unique_branches:\n",
    "    # Filter the DataFrame for the current branch and week 6\n",
    "    branch_data = df[(df['Company'] == branch) & (df['week'] == 4)]\n",
    "    # Calculate the required metrics\n",
    "    total_sales = branch_data['Total'].sum()\n",
    "    total_quantity_sold = branch_data['Qty Ordered'].sum()\n",
    "    unique_skus = branch_data['Product Variant'].nunique()\n",
    "    average_order_value = total_sales / branch_data.drop_duplicates(subset=['Order Date']).shape[0] if branch_data.shape[0] > 0 else 0\n",
    "    number_of_bills = branch_data.drop_duplicates(subset=['Order Date']).shape[0]\n",
    "    skus_per_bill = total_quantity_sold/number_of_bills\n",
    "    avg_daily_sales = total_sales / 7 if branch_data.shape[0] > 0 else 0\n",
    "    # Create a new sheet for each branch\n",
    "    ws = wb.create_sheet(title=f\"{branch} All Dates Metrics\")\n",
    "\n",
    "    # Write the metrics to the Excel sheet in the specified order\n",
    "    ws.append([\"Metric\", \"Value\"])\n",
    "    ws.append([\"Sales\", total_sales])\n",
    "    ws.append([\"Avg Daily Sales\", avg_daily_sales])\n",
    "    ws.append([\"AOV\", average_order_value])\n",
    "    ws.append([\"NOB\", number_of_bills])\n",
    "    ws.append([\"Qty Sold\", total_quantity_sold])\n",
    "    ws.append([\"Unique SKUs\", unique_skus])\n",
    "    ws.append([\"SKUs per bill\", skus_per_bill])\n",
    "\n",
    "    # Leave a blank row before the date-wise sales\n",
    "    ws.append([])\n",
    "\n",
    "    # Group by date and sum the sales across all dates\n",
    "    date_wise_sales = branch_data.groupby(branch_data['Order Date'].dt.date)['Total'].sum()\n",
    "\n",
    "    # Write the date-wise sales to the Excel sheet\n",
    "    ws.append([\"Date\", \"Sales\"])\n",
    "    for date, sales in date_wise_sales.items():\n",
    "        ws.append([date, sales])\n",
    "\n",
    "\n",
    "ws_top_sku = wb.create_sheet(title=\"Top SKU\")\n",
    "ws_top_sku.append(list(most_selling_sku_data.columns))  # Add column headers\n",
    "for row in most_selling_sku_data.itertuples(index=False):\n",
    "    ws_top_sku.append(row)  # Add data rows\n",
    "\n",
    "# Create a new sheet for \"Total Sales\" data\n",
    "ws_total_sales = wb.create_sheet(title=\"Total Sales\")\n",
    "ws_total_sales.append(list(branch_total_sales_df.columns))  # Add column headers\n",
    "for row in branch_total_sales_df.itertuples(index=False):\n",
    "    ws_total_sales.append(row)  # Add data rows\n",
    "\n",
    "# Create a new sheet for the comparison data\n",
    "ws_comparison = wb.create_sheet(title=\"Week 3 vs Week 4 Comparison\")\n",
    "ws_comparison.append(list(comparison_df.columns))\n",
    "for row in comparison_df.itertuples(index=False):\n",
    "    ws_comparison.append(row)\n",
    "\n",
    "# Create a new sheet for the weekly summary data\n",
    "ws_weekly_summary = wb.create_sheet(title=\"Weekly Summary\")\n",
    "ws_weekly_summary.append(list(weekly_summary_df.columns))\n",
    "for row in weekly_summary_df.itertuples(index=False):\n",
    "    ws_weekly_summary.append(row)\n",
    "\n",
    "ws_revenue_loss = wb.create_sheet(title=\"Revenue Loss Top 20\")\n",
    "ws_revenue_loss.append(['Product Variant', 'Revenue Loss'])  # Adding headers for Revenue Loss\n",
    "for row in top_50_revenue_loss_products_unique.itertuples(index=False, name=None):  # Use name=None for unnamed tuples\n",
    "    ws_revenue_loss.append([row[0], row[3]])  # Access by position (0-based index)\n",
    "\n",
    "# Create Revenue Gain sheet\n",
    "ws_revenue_gain = wb.create_sheet(title=\"Revenue Gain Top 20\")\n",
    "ws_revenue_gain.append(['Product Variant', 'Revenue Gain'])  # Adding headers for Revenue Gain\n",
    "\n",
    "for row in top_50_revenue_gain_products.itertuples(index=False, name=None):  # Use name=None for unnamed tuples\n",
    "    ws_revenue_gain.append([row[0], row[3]])  # Correctly reference column 4 ('Revenue Gain')\n",
    "\n",
    "ws_week_4 = wb.create_sheet(\"Week 4 Summary\")\n",
    "ws_week_4.append(list(summary.columns))\n",
    "for row in summary.itertuples(index=False):\n",
    "    ws_week_4.append(row)\n",
    "ws_week_4.append(list(summary1.columns))\n",
    "for row in summary1.itertuples(index=False):\n",
    "    ws_week_4.append(row)\n",
    "\n",
    "ws_week4 = wb.create_sheet(title=\"Top 20 Products - Week 4\")\n",
    "ws_week4.append(['Product Variant', 'Total Sales', 'Qty Ordered'])  # Adding headers\n",
    "for row in top_products_fourth_week.itertuples(name=None):  # Iterate without index\n",
    "    ws_week4.append([row[0], row[1], row[2]])  # Product Variant, Total Sales, Qty Ordered\n",
    "\n",
    "# Create \"Top 25 Products - Week 4 RJJ\" sheet\n",
    "ws_rjj = wb.create_sheet(title=\"Top 25 Products - Week 4 RJJ\")\n",
    "ws_rjj.append(['Product Variant', 'Total Sales', 'Qty Ordered'])  # Adding headers\n",
    "for row in top_products_rjj.itertuples(name=None):  # Iterate without index\n",
    "    ws_rjj.append([row[0], row[1], row[2]])  # Product Variant, Total Sales, Qty Ordered\n",
    "\n",
    "ws_summary = wb.create_sheet(title=\"Company Sales Summary - Week 4\")\n",
    "ws_summary.append([\n",
    "    'Company', \n",
    "    'Total Sales', \n",
    "    'Total Normal Sales', \n",
    "    'Total 50% Discount Sales', \n",
    "    'Total Normal Sales %', \n",
    "    'Total 50% Discount Sales %'\n",
    "])\n",
    "for row in company_sales_summary_week_4.itertuples(index=False, name=None):  # Use name=None for unnamed tuples\n",
    "    ws_summary.append(row)\n",
    "wb.save(file_path)\n",
    "\n",
    "print(f\"Data successfully added to {file_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8291f32a-458c-48a3-a756-bf0083aaea69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8d4be-7d7c-4c21-a788-a59273cf3015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd38314-6b41-45ac-b3fc-2ec69de1224d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
